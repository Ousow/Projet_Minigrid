# Exercices et D√©fis Suppl√©mentaires

Ce fichier contient des exercices optionnels pour approfondir votre compr√©hension de l'apprentissage par renforcement avec MiniGrid.

## üìö Niveau 1: Exp√©rimentations de base

### Exercice 1.1: Impact du Learning Rate

**Objectif:** Comprendre l'effet du taux d'apprentissage sur la convergence.

**T√¢che:**
1. Entra√Æner des agents Q-Learning avec diff√©rents learning rates: [0.01, 0.1, 0.5, 0.9]
2. Comparer les courbes d'apprentissage
3. Identifier le meilleur learning rate pour cet environnement

**Questions:**
- Que se passe-t-il avec un learning rate trop faible?
- Que se passe-t-il avec un learning rate trop √©lev√©?
- Comment choisir le bon learning rate?

**Code de d√©part:**
```python
learning_rates = [0.01, 0.1, 0.5, 0.9]
results = {}

for lr in learning_rates:
    agent = QLearningAgent(
        action_space_size=env.action_space.n,
        learning_rate=lr
    )
    # Entra√Æner et stocker les r√©sultats
    results[lr] = agent.train(env, num_episodes=500)

# Comparer les r√©sultats
```

### Exercice 1.2: Strat√©gies d'Exploration

**Objectif:** Tester diff√©rentes strat√©gies d'exploration.

**T√¢che:**
Impl√©menter et comparer:
1. **Epsilon-greedy** (d√©j√† impl√©ment√©)
2. **Epsilon-greedy avec d√©croissance lin√©aire** au lieu d'exponentielle
3. **Softmax/Boltzmann exploration**

**Code pour Boltzmann:**
```python
def select_action_boltzmann(self, state, temperature=1.0):
    state_key = self.state_to_key(state)
    q_values = self.q_table[state_key]
    
    # Calculer les probabilit√©s avec softmax
    exp_q = np.exp(q_values / temperature)
    probs = exp_q / np.sum(exp_q)
    
    return np.random.choice(self.action_space_size, p=probs)
```

**Questions:**
- Quelle strat√©gie converge le plus vite?
- Quelle strat√©gie est la plus stable?
- Comment le param√®tre de temp√©rature affecte-t-il l'exploration?

### Exercice 1.3: Fonction de R√©compense

**Objectif:** Comprendre l'impact du reward shaping.

**T√¢che:**
Modifier la r√©compense pour guider l'agent:

```python
# Exemple: p√©nalit√© pour chaque √©tape
def custom_reward(original_reward, done, steps):
    if done and original_reward > 0:
        return 1.0
    else:
        return -0.01  # P√©nalit√© pour encourager l'efficacit√©

# Ou: r√©compense bas√©e sur la distance √† l'objectif
def distance_reward(agent_pos, goal_pos):
    distance = np.sqrt((agent_pos[0] - goal_pos[0])**2 + 
                       (agent_pos[1] - goal_pos[1])**2)
    return -distance / 10
```

**Questions:**
- Le reward shaping aide-t-il l'apprentissage?
- Quels sont les risques du reward shaping?
- Comment √©viter de biaiser la politique?

## üöÄ Niveau 2: Am√©liorations algorithmiques

### Exercice 2.1: SARSA

**Objectif:** Impl√©menter SARSA et le comparer √† Q-Learning.

**Diff√©rence cl√©:**
- Q-Learning: utilise `max Q(s',a')` (off-policy)
- SARSA: utilise `Q(s',a')` o√π `a'` est l'action r√©ellement choisie (on-policy)

**Code √† modifier:**
```python
def update_sarsa(self, state, action, reward, next_state, next_action, done):
    state_key = self.state_to_key(state)
    next_state_key = self.state_to_key(next_state)
    
    current_q = self.q_table[state_key][action]
    next_q = 0 if done else self.q_table[next_state_key][next_action]
    
    new_q = current_q + self.learning_rate * (reward + self.gamma * next_q - current_q)
    self.q_table[state_key][action] = new_q
```

**Questions:**
- SARSA est-il plus conservateur que Q-Learning?
- Dans quels cas SARSA est-il pr√©f√©rable?
- Comparez les performances sur MiniGrid.

### Exercice 2.2: Double Q-Learning

**Objectif:** R√©duire le biais de surestimation.

**Principe:**
Utiliser deux tables Q et alterner entre elles.

**Code de d√©part:**
```python
class DoubleQLearningAgent:
    def __init__(self, ...):
        self.q_table_1 = defaultdict(lambda: np.zeros(action_space_size))
        self.q_table_2 = defaultdict(lambda: np.zeros(action_space_size))
    
    def update(self, state, action, reward, next_state, done):
        state_key = self.state_to_key(state)
        next_state_key = self.state_to_key(next_state)
        
        if np.random.random() < 0.5:
            # Mise √† jour Q1
            best_action = np.argmax(self.q_table_1[next_state_key])
            target = reward + self.gamma * self.q_table_2[next_state_key][best_action]
            self.q_table_1[state_key][action] += self.learning_rate * (target - self.q_table_1[state_key][action])
        else:
            # Mise √† jour Q2
            best_action = np.argmax(self.q_table_2[next_state_key])
            target = reward + self.gamma * self.q_table_1[next_state_key][best_action]
            self.q_table_2[state_key][action] += self.learning_rate * (target - self.q_table_2[state_key][action])
```

### Exercice 2.3: Prioritized Experience Replay

**Objectif:** Am√©liorer DQN avec replay buffer prioris√©.

**Principe:**
√âchantillonner les transitions importantes plus fr√©quemment.

**Code de d√©part:**
```python
class PrioritizedReplayBuffer:
    def __init__(self, capacity, alpha=0.6):
        self.capacity = capacity
        self.alpha = alpha  # Degr√© de priorisation
        self.buffer = []
        self.priorities = []
        self.pos = 0
    
    def push(self, transition):
        max_priority = max(self.priorities) if self.priorities else 1.0
        
        if len(self.buffer) < self.capacity:
            self.buffer.append(transition)
            self.priorities.append(max_priority)
        else:
            self.buffer[self.pos] = transition
            self.priorities[self.pos] = max_priority
        
        self.pos = (self.pos + 1) % self.capacity
    
    def sample(self, batch_size, beta=0.4):
        priorities = np.array(self.priorities)
        probs = priorities ** self.alpha
        probs /= probs.sum()
        
        indices = np.random.choice(len(self.buffer), batch_size, p=probs)
        samples = [self.buffer[idx] for idx in indices]
        
        # Calcul des poids d'importance
        weights = (len(self.buffer) * probs[indices]) ** (-beta)
        weights /= weights.max()
        
        return samples, indices, weights
    
    def update_priorities(self, indices, td_errors):
        for idx, error in zip(indices, td_errors):
            self.priorities[idx] = abs(error) + 1e-5
```

## üéØ Niveau 3: Environnements complexes

### Exercice 3.1: MiniGrid-DoorKey-8x8-v0

**Objectif:** Adapter vos agents √† un environnement avec portes et cl√©s.

**D√©fis:**
- Espace d'√©tats plus grand
- S√©quence d'actions requise (trouver cl√© ‚Üí ouvrir porte ‚Üí atteindre objectif)
- N√©cessite de la m√©moire/planification

**Modifications sugg√©r√©es:**
```python
env = gym.make('MiniGrid-DoorKey-8x8-v0')

# Augmenter le nombre d'√©pisodes
num_episodes = 5000

# Ajuster les hyperparam√®tres
learning_rate = 0.05  # Plus faible pour plus de stabilit√©
epsilon_decay = 0.999  # D√©croissance plus lente
```

**Questions:**
- Vos agents convergent-ils?
- Faut-il modifier la repr√©sentation de l'√©tat?
- Comment g√©rer la d√©pendance temporelle?

### Exercice 3.2: MiniGrid-FourRooms-v0

**Objectif:** Navigation dans un environnement avec obstacles.

**D√©fis:**
- Exploration difficile
- R√©compense sparse
- Besoin de traverser plusieurs pi√®ces

**Suggestion:**
Impl√©menter le **curiosity-driven exploration** avec une r√©compense intrins√®que.

```python
def intrinsic_reward(state_count, state):
    # R√©compenser la visite de nouveaux √©tats
    return 1.0 / np.sqrt(state_count[state])
```

### Exercice 3.3: MiniGrid avec obstacles dynamiques

**Objectif:** G√©rer un environnement non-stationnaire.

```python
env = gym.make('MiniGrid-Dynamic-Obstacles-8x8-v0')
```

**Questions:**
- L'agent apprend-il une politique robuste?
- Comment adapter l'exploration?
- Faut-il continuer √† explorer m√™me apr√®s convergence?

## üèÜ Niveau 4: D√©fis avanc√©s

### D√©fi 4.1: Meta-Learning

**Objectif:** Entra√Æner un agent capable de s'adapter rapidement √† de nouveaux environnements.

**Approche:**
1. Entra√Æner sur plusieurs environnements MiniGrid
2. Utiliser les param√®tres appris comme initialisation
3. Fine-tuner rapidement sur un nouvel environnement

### D√©fi 4.2: Hierarchical RL

**Objectif:** D√©composer la t√¢che en sous-t√¢ches.

**Exemple pour DoorKey:**
- Macro-action 1: Trouver la cl√©
- Macro-action 2: Aller √† la porte
- Macro-action 3: Atteindre l'objectif

### D√©fi 4.3: Imitation Learning

**Objectif:** Pr√©-entra√Æner avec des d√©monstrations humaines.

**√âtapes:**
1. Enregistrer des trajectoires optimales
2. Pr√©-entra√Æner avec Behavioral Cloning
3. Fine-tuner avec RL

### D√©fi 4.4: Multi-Agent RL

**Objectif:** Plusieurs agents coop√©ratifs ou comp√©titifs.

```python
env = gym.make('MultiGrid-CompetativeRedBlueDoor-v0')
```

## üìä Exercice d'Analyse

### Analyse comparative compl√®te

**Objectif:** Produire une analyse scientifique rigoureuse.

**T√¢ches:**
1. Ex√©cuter 10 seeds diff√©rents pour chaque algorithme
2. Calculer moyenne et intervalle de confiance
3. Tests statistiques (t-test, Mann-Whitney)
4. Analyse de la variance
5. Graphiques avec barres d'erreur

**Code de d√©part:**
```python
from scipy import stats

results_qlearning = []
results_dqn = []

for seed in range(10):
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    # Entra√Æner Q-Learning
    agent_q = QLearningAgent(...)
    rewards_q = agent_q.train(env, ...)
    results_qlearning.append(np.mean(rewards_q[-100:]))
    
    # Entra√Æner DQN
    agent_dqn = DQNAgent(...)
    rewards_dqn = agent_dqn.train(env, ...)
    results_dqn.append(np.mean(rewards_dqn[-100:]))

# Test statistique
t_stat, p_value = stats.ttest_ind(results_qlearning, results_dqn)
print(f"t-statistic: {t_stat}, p-value: {p_value}")

# Intervalles de confiance √† 95%
conf_interval_q = stats.t.interval(0.95, len(results_qlearning)-1,
                                    loc=np.mean(results_qlearning),
                                    scale=stats.sem(results_qlearning))
```

## üéì Projet Final Sugg√©r√©

### Cr√©er votre propre environnement MiniGrid

**Objectif:** Concevoir et r√©soudre un environnement personnalis√©.

**√âtapes:**
1. D√©finir la t√¢che (ex: collecte d'objets, puzzle)
2. Cr√©er l'environnement avec MiniGrid
3. Adapter vos agents
4. Analyser les r√©sultats
5. Publier sur GitHub

**Template:**
```python
from minigrid.core.grid import Grid
from minigrid.core.world_object import Goal, Wall
from minigrid.minigrid_env import MiniGridEnv

class CustomEnv(MiniGridEnv):
    def __init__(self, size=8, **kwargs):
        self.size = size
        super().__init__(
            grid_size=size,
            max_steps=4 * size * size,
            **kwargs
        )
    
    def _gen_grid(self, width, height):
        # Cr√©er la grille
        self.grid = Grid(width, height)
        
        # Ajouter les murs
        self.grid.wall_rect(0, 0, width, height)
        
        # Placer l'agent
        self.agent_pos = (1, 1)
        self.agent_dir = 0
        
        # Placer l'objectif
        self.put_obj(Goal(), width - 2, height - 2)
        
        # Ajouter votre logique personnalis√©e
        # ...

# Utiliser l'environnement
from gymnasium.envs.registration import register

register(
    id='MiniGrid-Custom-v0',
    entry_point='__main__:CustomEnv'
)

env = gym.make('MiniGrid-Custom-v0')
```

## üìù Checklist des Exercices

- [ ] Exercice 1.1: Impact du Learning Rate
- [ ] Exercice 1.2: Strat√©gies d'Exploration
- [ ] Exercice 1.3: Fonction de R√©compense
- [ ] Exercice 2.1: SARSA
- [ ] Exercice 2.2: Double Q-Learning
- [ ] Exercice 2.3: Prioritized Experience Replay
- [ ] Exercice 3.1: DoorKey Environment
- [ ] Exercice 3.2: FourRooms Environment
- [ ] Exercice 3.3: Dynamic Obstacles
- [ ] D√©fi 4.1: Meta-Learning
- [ ] D√©fi 4.2: Hierarchical RL
- [ ] D√©fi 4.3: Imitation Learning
- [ ] D√©fi 4.4: Multi-Agent RL
- [ ] Analyse comparative compl√®te
- [ ] Projet final personnalis√©

## üéØ Crit√®res d'√âvaluation

Pour chaque exercice, √©valuez-vous selon:

1. **Compr√©hension** (30%)
   - Comprenez-vous le concept?
   - Pouvez-vous l'expliquer?

2. **Impl√©mentation** (40%)
   - Code fonctionnel?
   - Bonnes pratiques?
   - Commentaires clairs?

3. **Analyse** (30%)
   - R√©sultats interpr√©t√©s?
   - Comparaisons pertinentes?
   - Conclusions justifi√©es?

## üí° Conseils

1. **Commencez simple:** Validez chaque modification avant de passer √† la suivante
2. **Documentez:** Notez tous vos r√©sultats et observations
3. **Visualisez:** Les graphiques aident √† comprendre
4. **Comparez:** Toujours avoir une baseline
5. **It√©rez:** L'apprentissage par renforcement n√©cessite de l'exp√©rimentation

Bon courage! üöÄ
