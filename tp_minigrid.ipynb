{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Apprentissage par Renforcement avec MiniGrid\n",
    "\n",
    "**Auteur:** [Votre Nom]  \n",
    "**Date:** F√©vrier 2026\n",
    "\n",
    "Ce notebook vous guide √† travers l'impl√©mentation et la comparaison de Q-Learning et DQN sur MiniGrid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì¶ Installation et Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des d√©pendances (d√©commentez si n√©cessaire)\n",
    "# !pip install gymnasium minigrid numpy matplotlib torch tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import minigrid\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from collections import defaultdict, deque\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "\n",
    "print(\"‚úì Imports r√©ussis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Partie 1: Exploration de MiniGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er l'environnement\n",
    "env = gym.make('MiniGrid-Empty-8x8-v0', render_mode='rgb_array')\n",
    "\n",
    "print(\"Informations sur l'environnement:\")\n",
    "print(f\"Espace d'observation: {env.observation_space}\")\n",
    "print(f\"Espace d'actions: {env.action_space}\")\n",
    "print(f\"Nombre d'actions: {env.action_space.n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions disponibles\n",
    "actions = {\n",
    "    0: \"Tourner √† gauche\",\n",
    "    1: \"Tourner √† droite\",\n",
    "    2: \"Avancer\",\n",
    "    3: \"Ramasser\",\n",
    "    4: \"D√©poser\",\n",
    "    5: \"Basculer\",\n",
    "    6: \"Terminer\"\n",
    "}\n",
    "\n",
    "for action_id, description in actions.items():\n",
    "    if action_id < env.action_space.n:\n",
    "        print(f\"Action {action_id}: {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser l'environnement\n",
    "obs, info = env.reset()\n",
    "img = env.render()\n",
    "\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(img)\n",
    "plt.title(\"MiniGrid-Empty-8x8-v0\")\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nStructure de l'observation:\")\n",
    "if isinstance(obs, dict):\n",
    "    for key, value in obs.items():\n",
    "        if isinstance(value, np.ndarray):\n",
    "            print(f\"{key}: shape={value.shape}\")\n",
    "        else:\n",
    "            print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avec des actions al√©atoires\n",
    "env.reset()\n",
    "total_reward = 0\n",
    "\n",
    "for step in range(50):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    \n",
    "    if terminated or truncated:\n",
    "        print(f\"√âpisode termin√© √† l'√©tape {step+1}\")\n",
    "        print(f\"R√©compense totale: {total_reward}\")\n",
    "        break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† Partie 2: Impl√©mentation Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"Agent Q-Learning simplifi√© pour notebook.\"\"\"\n",
    "    \n",
    "    def __init__(self, action_space_size, learning_rate=0.1, gamma=0.99, \n",
    "                 epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.995):\n",
    "        self.action_space_size = action_space_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.q_table = defaultdict(lambda: np.zeros(action_space_size))\n",
    "        \n",
    "    def state_to_key(self, obs):\n",
    "        if isinstance(obs, dict) and 'image' in obs:\n",
    "            return tuple(obs['image'].flatten())\n",
    "        return tuple(np.array(obs).flatten())\n",
    "    \n",
    "    def select_action(self, state, training=True):\n",
    "        if training and np.random.random() < self.epsilon:\n",
    "            return np.random.randint(self.action_space_size)\n",
    "        state_key = self.state_to_key(state)\n",
    "        return np.argmax(self.q_table[state_key])\n",
    "    \n",
    "    def update(self, state, action, reward, next_state, done):\n",
    "        state_key = self.state_to_key(state)\n",
    "        next_state_key = self.state_to_key(next_state)\n",
    "        \n",
    "        current_q = self.q_table[state_key][action]\n",
    "        max_next_q = 0 if done else np.max(self.q_table[next_state_key])\n",
    "        new_q = current_q + self.learning_rate * (reward + self.gamma * max_next_q - current_q)\n",
    "        self.q_table[state_key][action] = new_q\n",
    "    \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "print(\"‚úì Classe QLearningAgent d√©finie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entra√Æner l'agent Q-Learning\n",
    "env = gym.make('MiniGrid-Empty-8x8-v0')\n",
    "agent_qlearning = QLearningAgent(env.action_space.n)\n",
    "\n",
    "num_episodes = 1000\n",
    "rewards_qlearning = []\n",
    "steps_qlearning = []\n",
    "\n",
    "for episode in tqdm(range(num_episodes), desc=\"Entra√Ænement Q-Learning\"):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    \n",
    "    for step in range(500):\n",
    "        action = agent_qlearning.select_action(state)\n",
    "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        agent_qlearning.update(state, action, reward, next_state, done)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        episode_steps += 1\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    agent_qlearning.decay_epsilon()\n",
    "    rewards_qlearning.append(episode_reward)\n",
    "    steps_qlearning.append(episode_steps)\n",
    "\n",
    "env.close()\n",
    "print(\"‚úì Entra√Ænement Q-Learning termin√©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser les r√©sultats Q-Learning\n",
    "window = 100\n",
    "moving_avg = np.convolve(rewards_qlearning, np.ones(window)/window, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards_qlearning, alpha=0.3, label='R√©compense')\n",
    "plt.plot(range(window-1, len(rewards_qlearning)), moving_avg, \n",
    "         color='red', label=f'Moyenne mobile ({window})')\n",
    "plt.xlabel('√âpisode')\n",
    "plt.ylabel('R√©compense')\n",
    "plt.title('Q-Learning - R√©compenses')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "moving_avg_steps = np.convolve(steps_qlearning, np.ones(window)/window, mode='valid')\n",
    "plt.plot(steps_qlearning, alpha=0.3, label='√âtapes')\n",
    "plt.plot(range(window-1, len(steps_qlearning)), moving_avg_steps,\n",
    "         color='orange', label=f'Moyenne mobile ({window})')\n",
    "plt.xlabel('√âpisode')\n",
    "plt.ylabel('Nombre d\\'√©tapes')\n",
    "plt.title('Q-Learning - √âtapes par √©pisode')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"R√©compense moyenne (derniers 100 √©p.): {np.mean(rewards_qlearning[-100:]):.3f}\")\n",
    "print(f\"√âtapes moyennes (derniers 100 √©p.): {np.mean(steps_qlearning[-100:]):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ü§ñ Partie 3: Impl√©mentation DQN\n",
    "\n",
    "*(Note: L'impl√©mentation compl√®te de DQN est dans 3_dqn_agent.py)*\n",
    "\n",
    "Pour utiliser DQN dans ce notebook, vous pouvez importer la classe depuis le fichier Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Partie 4: √âvaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# √âvaluer Q-Learning\n",
    "env = gym.make('MiniGrid-Empty-8x8-v0')\n",
    "eval_rewards = []\n",
    "eval_steps = []\n",
    "successes = 0\n",
    "\n",
    "for episode in range(100):\n",
    "    state, _ = env.reset()\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    \n",
    "    for step in range(500):\n",
    "        action = agent_qlearning.select_action(state, training=False)\n",
    "        state, reward, terminated, truncated, _ = env.step(action)\n",
    "        \n",
    "        episode_reward += reward\n",
    "        episode_steps += 1\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            if reward > 0:\n",
    "                successes += 1\n",
    "            break\n",
    "    \n",
    "    eval_rewards.append(episode_reward)\n",
    "    eval_steps.append(episode_steps)\n",
    "\n",
    "env.close()\n",
    "\n",
    "print(\"\\n=== R√âSULTATS D'√âVALUATION Q-LEARNING ===\")\n",
    "print(f\"R√©compense moyenne: {np.mean(eval_rewards):.3f} ¬± {np.std(eval_rewards):.3f}\")\n",
    "print(f\"√âtapes moyennes: {np.mean(eval_steps):.1f} ¬± {np.std(eval_steps):.1f}\")\n",
    "print(f\"Taux de succ√®s: {successes}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des r√©compenses\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(eval_rewards, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(np.mean(eval_rewards), color='red', linestyle='--', \n",
    "            label=f'Moyenne: {np.mean(eval_rewards):.2f}')\n",
    "plt.xlabel('R√©compense')\n",
    "plt.ylabel('Fr√©quence')\n",
    "plt.title('Distribution des r√©compenses')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(eval_steps, bins=20, edgecolor='black', alpha=0.7, color='orange')\n",
    "plt.axvline(np.mean(eval_steps), color='red', linestyle='--',\n",
    "            label=f'Moyenne: {np.mean(eval_steps):.1f}')\n",
    "plt.xlabel('Nombre d\\'√©tapes')\n",
    "plt.ylabel('Fr√©quence')\n",
    "plt.title('Distribution des √©tapes')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üíæ Sauvegarde des r√©sultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder l'agent\n",
    "with open('qlearning_agent.pkl', 'wb') as f:\n",
    "    pickle.dump({\n",
    "        'q_table': dict(agent_qlearning.q_table),\n",
    "        'training_rewards': rewards_qlearning,\n",
    "        'training_steps': steps_qlearning,\n",
    "        'eval_rewards': eval_rewards,\n",
    "        'eval_steps': eval_steps\n",
    "    }, f)\n",
    "\n",
    "print(\"‚úì Agent sauvegard√© dans 'qlearning_agent.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Conclusions\n",
    "\n",
    "### √Ä compl√©ter:\n",
    "\n",
    "1. **Performance observ√©e:**\n",
    "   - Taux de succ√®s: ____%\n",
    "   - R√©compense moyenne: ____\n",
    "   - Convergence apr√®s ____ √©pisodes\n",
    "\n",
    "2. **Analyse:**\n",
    "   - Points forts de Q-Learning:\n",
    "   - Limitations observ√©es:\n",
    "   - Comparaison avec DQN:\n",
    "\n",
    "3. **Am√©liorations possibles:**\n",
    "   - \n",
    "   - \n",
    "   - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Prochaines √©tapes\n",
    "\n",
    "1. Impl√©menter DQN (voir `3_dqn_agent.py`)\n",
    "2. Comparer les deux algorithmes\n",
    "3. Tester sur des environnements plus complexes\n",
    "4. Exp√©rimenter avec les hyperparam√®tres\n",
    "5. Compl√©ter le rapport final"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
